![Image](https://www.itwonderlab.com/wp-content/uploads/tutorials/Kubernetes-NFS/Kubernetes_PVC_NFS.png)

![Image](https://matthewpalmer.net/kubernetes-app-developer/articles/volume-mount-diagram.png)

Below is a **clean, corrected, and production-friendly version** of your NFS volume setup in Kubernetes.
Iâ€™ve also **highlighted common mistakes and missing best practices** clearly.

---

# ğŸ“¦ Mounting an NFS Volume in Kubernetes (Beginner-Friendly)

Kubernetes does **not create NFS storage automatically**.
You must **pre-create NFS storage** and then connect it to Pods using **PV â†’ PVC â†’ Pod**.

---

## ğŸŸ¢ Step 0: Prerequisites (Very Important)

Before Kubernetes YAMLs:

* NFS server must be **running**
* Export path must exist
* Worker nodes must be able to **reach NFS server**
* Install NFS client on nodes:

```bash
sudo apt install nfs-common -y   # Ubuntu
sudo yum install nfs-utils -y    # RHEL/Amazon Linux
```

âš ï¸ **Common mistake:**
PVC stuck in `Pending` because NFS client is missing on nodes.

---

## ğŸŸ¢ Step 1: Create PersistentVolume (PV)

ğŸ“„ `nfs-pv.yaml`

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
spec:
  storageClassName: nfs-storage
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 192.168.1.100   # NFS server IP
    path: /exports/data     # Exported path
```

### Explanation

* **PV** = actual storage
* `ReadWriteMany` â†’ multiple Pods can mount
* `Retain` â†’ data not deleted if PVC is deleted

ğŸ§  **Analogy:**
PV = **Shared network drive**

---

## ğŸŸ¢ Step 2: Create PersistentVolumeClaim (PVC)

ğŸ“„ `nfs-pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-storage
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
```

### Explanation

* PVC is a **request** for storage
* Must match:

  * `storageClassName`
  * `accessModes`

ğŸ§  **Analogy:**
PVC = **Request form for shared storage**

âš ï¸ **Common mistake:**
If storageClassName does not match â†’ PVC stays `Pending`.

---

## ğŸŸ¢ Step 3: Create Pod Using NFS PVC

ğŸ“„ `nfs-pod.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nfs-pod
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - name: nfs-volume
          mountPath: /usr/share/nginx/html
  volumes:
    - name: nfs-volume
      persistentVolumeClaim:
        claimName: nfs-pvc
```

### Explanation

* PVC is mounted inside container
* Data written here is stored on **NFS server**
* Pod restart âŒ data loss â†’ **NO**

ğŸ§  **Analogy:**
Pod using NFS = **Multiple laptops using same Google Drive**

---

## ğŸŸ¢ Step 4: Apply Resources (Correct Order)

```bash
kubectl apply -f nfs-pv.yaml
kubectl apply -f nfs-pvc.yaml
kubectl apply -f nfs-pod.yaml
```

Verify:

```bash
kubectl get pv
kubectl get pvc
kubectl get pods
```

Expected:

* PV â†’ `Bound`
* PVC â†’ `Bound`
* Pod â†’ `Running`

---

## âŒ Common Mistakes (Very Important)

### âŒ Wrong server IP

* Pod stuck in `ContainerCreating`
* Fix: Verify NFS server IP & firewall

---

### âŒ Missing RWX support

* Using cloud disk (EBS) instead of NFS
* RWX works only with NFS / EFS / Azure Files

---

### âŒ No `Retain` policy

* Data deleted accidentally when PVC deleted

---

### âŒ Using Pod instead of Deployment

* Pods are not self-healing
* Use Deployment/StatefulSet in real projects

---

## ğŸŸ¡ Production Best Practices (Extra Knowledge)

âœ… Use **StatefulSet** for databases
âœ… Use **NFS CSI driver** instead of static PV
âœ… Add **SecurityContext** if permission issues
âœ… Monitor NFS performance (latency sensitive)

---

## ğŸ”‘ One-Line Interview Answer

> â€œTo mount NFS in Kubernetes, we create a PersistentVolume pointing to the NFS server, bind it using a PVC, and mount that PVC inside a Pod. This provides shared, persistent storage.â€

---

## â­ Interview Tip

If interviewer asks **â€œWhy NFS?â€**, say:

> â€œNFS supports ReadWriteMany, so multiple Pods can share the same data across nodes.â€

---

![Image](https://hpe-developer-portal.s3.amazonaws.com/uploads/media/2020/6/csi-120-rev1b-1592687082644.png)

![Image](https://www.simplyblock.io/wp-content/media/Dynamic-Provisioning-in-Kubernetes.png)

Below is the **PRODUCTION-READY way to use NFS in Kubernetes using the CSI driver**.
This is how itâ€™s done in **real projects**, not lab-style static PVs.

---

# ğŸš€ NFS CSI Driver (Production Way â€“ Simple English)

## Why CSI instead of normal NFS PV?

âŒ Static PV = manual, not scalable
âœ… **CSI Driver = dynamic provisioning, production-ready**

ğŸ§  **Analogy:**
Static PV = manually booking rooms
CSI Driver = **hotel reception auto-assigns rooms**

---

## ğŸŸ¢ Architecture (High Level)

```
Pod â†’ PVC â†’ StorageClass â†’ NFS CSI Driver â†’ NFS Server
```

* CSI driver talks to NFS
* Storage is created **automatically**
* No need to create PV manually

---

## ğŸŸ¢ Step 0: Prerequisites

* NFS server running
* Export path exists (example: `/exports/k8s`)
* Worker nodes have NFS client installed:

```bash
sudo apt install nfs-common -y
```

---

## ğŸŸ¢ Step 1: Install NFS CSI Driver

Official driver: **nfs.csi.k8s.io**

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/install-driver.yaml
```

Verify:

```bash
kubectl get pods -n kube-system | grep nfs
```

Expected:

* `csi-nfs-controller`
* `csi-nfs-node`

ğŸ§  **Analogy:**
CSI driver = **Translator between Kubernetes and NFS**

---

## ğŸŸ¢ Step 2: Create StorageClass (MOST IMPORTANT)

ğŸ“„ `nfs-storageclass.yaml`

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-csi
provisioner: nfs.csi.k8s.io
parameters:
  server: 192.168.1.100        # NFS server IP
  share: /exports/k8s          # NFS export path
reclaimPolicy: Retain
volumeBindingMode: Immediate
```

Apply:

```bash
kubectl apply -f nfs-storageclass.yaml
```

ğŸ§  **Analogy:**
StorageClass = **Rule book for creating storage**

---

## ğŸŸ¢ Step 3: Create PVC (Dynamic â€“ No PV!)

ğŸ“„ `nfs-pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-pvc
spec:
  storageClassName: nfs-csi
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
```

Apply:

```bash
kubectl apply -f nfs-pvc.yaml
```

Check:

```bash
kubectl get pvc
kubectl get pv
```

âœ… PV is **AUTO-CREATED**

ğŸ§  **Analogy:**
PVC = **Online order**, PV = **auto-delivery**

---

## ğŸŸ¢ Step 4: Use PVC in Deployment / StatefulSet

ğŸ“„ `nginx-deploy.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: nfs-vol
              mountPath: /usr/share/nginx/html
      volumes:
        - name: nfs-vol
          persistentVolumeClaim:
            claimName: nfs-pvc
```

Apply:

```bash
kubectl apply -f nginx-deploy.yaml
```

ğŸ§  **Analogy:**
Multiple Pods = **multiple laptops using same shared drive**

---

## ğŸŸ¢ Step 5: Verify (Production Checks)

```bash
kubectl get pods
kubectl describe pvc nfs-pvc
kubectl exec -it <pod-name> -- df -h
```

* Data shared across Pods
* Pod restart âŒ data loss â†’ **NO**
* Node change âŒ issue â†’ **NO**

---

## ğŸ” Production Best Practices (VERY IMPORTANT)

âœ… Use **RWX** only when needed
âœ… Use **StatefulSet** for DBs
âœ… Use **Retain** reclaim policy
âœ… Secure NFS exports (IP allowlist)
âœ… Monitor latency (NFS is network-based)

---

## âŒ Common Production Mistakes

âŒ Using HostPath in prod
âŒ Using static PV for dynamic workloads
âŒ Forgetting NFS client on nodes
âŒ Using NFS for high-IO databases blindly

---

## ğŸ”‘ Interview-Ready One-Line Answer

> â€œIn production, NFS is integrated using the NFS CSI driver with a StorageClass, enabling dynamic provisioning of RWX volumes without manual PV creation.â€

---

## â­ Interview Tip

If asked **â€œStatic PV vs CSI?â€**, say:

> â€œStatic PVs are manual and not scalable, whereas CSI drivers enable dynamic, cloud-native storage provisioning.â€

---

![Image](https://hpe-developer-portal.s3.amazonaws.com/uploads/media/2020/6/csi-120-rev1b-1592687082644.png)

![Image](https://www.opcito.com/sites/default/files/inline-images/EFS%20CSI%20Driver%20.png)

![Image](https://miro.medium.com/1%2AGig18TiVgWdxXeedc778UA.png)

Below is a **clear, interview-ready comparison of NFS CSI vs EFS CSI**, written in **simple English**, with **real-world usage** and **easy analogy**.

---

# ğŸ” NFS CSI vs EFS CSI (Kubernetes Storage)

## ğŸ”¹ What they are (1 line each)

### NFS CSI

* Uses **your own NFS server** (on-prem or VM) as shared storage.

ğŸ§  Analogy:
**Office file server** inside your company.

---

### EFS CSI

* Uses **AWS managed file system (EFS)** as shared storage.

ğŸ§  Analogy:
**Google Drive managed by AWS**.

---

## ğŸ”¸ Core Comparison Table

| Feature           | NFS CSI Driver        | EFS CSI Driver      |
| ----------------- | --------------------- | ------------------- |
| Storage Type      | Self-managed NFS      | AWS managed EFS     |
| Cloud Dependency  | Any (on-prem / cloud) | AWS only            |
| Setup Effort      | Medium / High         | Very low            |
| Maintenance       | You manage server     | AWS manages         |
| RWX Support       | âœ… Yes                 | âœ… Yes               |
| High Availability | âŒ Manual              | âœ… Built-in          |
| Scaling           | âŒ Manual              | âœ… Automatic         |
| Cost              | Low                   | Higher              |
| Performance       | Depends on server     | High & elastic      |
| Production Ready  | âš ï¸ Yes (with care)    | âœ… Yes (recommended) |

---

## ğŸ”¹ Architecture Difference (Simple)

### NFS CSI

```
Pod â†’ PVC â†’ StorageClass â†’ NFS CSI â†’ Your NFS Server
```

### EFS CSI

```
Pod â†’ PVC â†’ StorageClass â†’ EFS CSI â†’ AWS EFS (Multi-AZ)
```

---

## ğŸ”¹ When to Use NFS CSI

âœ… On-prem Kubernetes
âœ… Budget-sensitive projects
âœ… Dev / Test environments
âœ… Small to medium workloads

âŒ Not ideal for:

* Large scale
* Auto scaling
* Multi-AZ without effort

ğŸ§  Analogy:
NFS CSI = **You own the fridge and fix it yourself**

---

## ğŸ”¹ When to Use EFS CSI

âœ… AWS EKS production
âœ… Microservices needing shared storage
âœ… Multi-AZ workloads
âœ… Zero maintenance storage

âŒ Not ideal for:

* Tight budgets
* Non-AWS clusters

ğŸ§  Analogy:
EFS CSI = **AWS provides fridge, power, backup, and repair**

---

## ğŸ”¹ Operational Differences (Real Projects)

### NFS CSI (Ops team responsibilities)

* Patch NFS server
* Monitor disk usage
* Handle HA & backup
* Manage performance

### EFS CSI (AWS handles)

* HA across AZs
* Auto scaling storage
* Durability & backup
* Security integration (IAM)

---

## ğŸ”¹ Performance Consideration

* **NFS CSI** â†’ performance limited by server size
* **EFS CSI** â†’ scales automatically, better for many Pods

ğŸ§  Analogy:
NFS = **Single water tank**
EFS = **City water supply**

---

## ğŸ”‘ Interview One-Line Answers

**Q: NFS CSI vs EFS CSI?**

> â€œNFS CSI is self-managed and flexible, while EFS CSI is AWS-managed, highly available, and best for production EKS workloads.â€

**Q: Which is better for production?**

> â€œEFS CSI is preferred in AWS production due to built-in HA, scalability, and low ops overhead.â€

---

## â­ Final Recommendation (Real World)

* ğŸ§ª **Labs / Dev / On-Prem** â†’ NFS CSI
* ğŸš€ **AWS EKS Production** â†’ EFS CSI

---

![Image](https://www.opcito.com/sites/default/files/inline-images/EFS%20CSI%20Driver%20.png)

![Image](https://www.eksworkshop.com/assets/images/efs-storage-ffc3bb4d0971fa00f17956d14ecd3b71.webp)

![Image](https://docs.aws.amazon.com/images/prescriptive-guidance/latest/patterns/images/pattern-img/2487e285-269b-415b-a270-877f973e3aaf/images/ec8de63c-3307-4010-9e03-2bd7b9881fff.png)

Below is a **FULL, PRODUCTION-READY EFS CSI setup using YAML**, exactly how itâ€™s done in **real AWS EKS projects**, written in **simple English** and **step-by-step**.

---

# ğŸš€ AWS EFS CSI Driver â€“ Full Setup (Production Way)

## ğŸŸ¢ Prerequisites (Must Have)

* âœ… Kubernetes cluster on **AWS EKS**
* âœ… **EFS File System** already created
* âœ… EFS mounted in **same VPC** as EKS
* âœ… Worker nodes have **security group access to EFS (2049)**

ğŸ§  **Analogy:**
EFS = **AWS-managed shared hard disk**

---

## ğŸŸ¢ Step 1: Create IAM Policy for EFS CSI

ğŸ“„ `efs-csi-policy.json`

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:DescribeAccessPoints",
        "elasticfilesystem:DescribeFileSystems",
        "elasticfilesystem:CreateAccessPoint",
        "elasticfilesystem:DeleteAccessPoint"
      ],
      "Resource": "*"
    }
  ]
}
```

Attach this policy to:

* **Node IAM role**
  (or)
* **IRSA service account** (recommended)

ğŸ§  **Analogy:**
IAM policy = **Permission letter to use storage**

---

## ğŸŸ¢ Step 2: Install EFS CSI Driver

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/deploy/kubernetes/base/kustomization.yaml
```

Verify:

```bash
kubectl get pods -n kube-system | grep efs
```

Expected:

* `efs-csi-controller`
* `efs-csi-node`

ğŸ§  **Analogy:**
CSI Driver = **Translator between Kubernetes and AWS EFS**

---

## ğŸŸ¢ Step 3: Create StorageClass (Dynamic Provisioning)

ğŸ“„ `efs-storageclass.yaml`

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-0abcd1234ef567890   # YOUR EFS ID
  directoryPerms: "700"
reclaimPolicy: Retain
volumeBindingMode: Immediate
```

Apply:

```bash
kubectl apply -f efs-storageclass.yaml
```

ğŸ§  **Analogy:**
StorageClass = **Rules on how AWS should create folders**

---

## ğŸŸ¢ Step 4: Create PersistentVolumeClaim (PVC)

ğŸ“„ `efs-pvc.yaml`

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 5Gi
```

Apply:

```bash
kubectl apply -f efs-pvc.yaml
```

Check:

```bash
kubectl get pvc
kubectl get pv
```

âœ… PV is **auto-created** by EFS CSI

ğŸ§  **Analogy:**
PVC = **Requesting a shared AWS folder**

---

## ğŸŸ¢ Step 5: Use EFS PVC in Deployment

ğŸ“„ `nginx-efs-deploy.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-efs
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx-efs
  template:
    metadata:
      labels:
        app: nginx-efs
    spec:
      containers:
        - name: nginx
          image: nginx
          volumeMounts:
            - name: efs-volume
              mountPath: /usr/share/nginx/html
      volumes:
        - name: efs-volume
          persistentVolumeClaim:
            claimName: efs-pvc
```

Apply:

```bash
kubectl apply -f nginx-efs-deploy.yaml
```

ğŸ§  **Analogy:**
Multiple Pods = **Multiple laptops using same Google Drive**

---

## ğŸŸ¢ Step 6: Verification

```bash
kubectl get pods
kubectl exec -it <pod-name> -- df -h
```

* Write a file in one Pod
* Read it from another Pod
  âœ… Data shared successfully

---

## ğŸ” Production Best Practices (IMPORTANT)

âœ… Use **IRSA** instead of node IAM
âœ… Use **StatefulSet** for DB workloads
âœ… Keep **Retain** reclaim policy
âœ… Secure EFS with **SG + NACL**
âœ… Monitor EFS throughput & latency

---

## âŒ Common Mistakes

âŒ Missing port **2049** in security group
âŒ Wrong **fileSystemId**
âŒ Using EBS when RWX is required
âŒ Forgetting IAM permissions

---

## ğŸ”‘ Interview-Ready One-Liner

> â€œIn AWS EKS production, we use the EFS CSI driver with a StorageClass to dynamically provision RWX volumes backed by AWS-managed EFS.â€

---

## â­ Interview Tip

If asked **â€œWhy EFS CSI?â€**, say:

> â€œEFS CSI provides shared, highly available, multi-AZ storage with zero maintenance.â€

---
